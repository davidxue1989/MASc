\chapter{Visual Focus of Attention Estimation}

To track the VFOA of the child, we are using the Microsoft Kinect to collect RGB and depth images of the child's head.  Gaze directions can be tracked more accurately when head is viewed near frontal, and since we cared more about the child's attention to prompting agents than to sink objects, we setup the Kinect near the robot, and have sink objects further from the Kinect (i.e. soap and towel) be spread apart from each other to be easily distinguished.

The problem of estimating the object under child's attentional focus is broken into three parts: head pose estimation, eye pose estimation, and object identification.


%Overall objective
%	- to implement a real time gaze tracker
%a short reason why, so we know what's good enough and what's important to focus on
%
%2D webcam Approach:
%	- BLAH face tracker, eye corner extraction, stretch based inverse pose transformation, evaluation
%	
%	
%3D Kinect camera approach:
%	- KinFu mesh building, ICP head pose tracking, point cloud based inverse transformation, evaluation
%	- challenges of children with ASD footage, future algorithm requirement



\input{./tex/gaze_tracking_chp/head_pose_estimation}
\input{./tex/gaze_tracking_chp/eye_pose_estimation}
\input{./tex/gaze_tracking_chp/object_identification}

\section{Discussion}
We have shown working implementations of the KinFu algorithm that produces inverse head pose transformed color eye images.  However, due to limitation of time, its capability was not explored.  We did test it with Kinect video footages we've obtained of the child with ASD's head during his hand-washing trials.  However, because the Kinect camera was placed too close to the participant's face (due to limitation of space near the sink), and because the participant rocks back and forth quite rapidly, our algorithm could not track the participant's head motions robustly.  Thus, we weren't able to build a head model from the footages, and thus further investigation was impeded.  For future studies, we propose characterizing the amount of motion the head tracking algorithm can handle, as well as characterizing the typical amount of head motion children with ASD exhibit during the hand-washing trials.  Because of the head tracking algorithm not working with the child with ASD, as well as due to limitation of time, we did not proceed in developing the eye pose tracking algorithm.  However, a simple object location calibration method was proposed and implemented with success.