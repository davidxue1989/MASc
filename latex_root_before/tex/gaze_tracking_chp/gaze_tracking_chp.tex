\chapter{Visual Focus of Attention Estimation}
From the WoZ pilot study, we have learned that our participant did not look at the robot as often as expected during prompting.  This meant either that the robot and its actions were not visually stimulating enough to motivate the participant to focus his attention upon or that he was not interested in observing the motions demonstrated possibly because he already knows them.  Although no relationships between the Visual Focus of Attention (VFOA on robot and Compliance Rate were observed for our participant, literatures suggest that keeping the child with ASD visually engaged during action modeling when teaching a new skill is essential [\ref{Sec:AT4ASDDiscussion}].  Thus, for future studies with participants that are not familiar with the hand-washing motions, it is still helpful to be able to track the VFOA of the child with ASD during the hand-washing activity, so that the robot actions may adjust automatically to better engage the child's visual attention.

To track the VFOA of the child, we used the Microsoft Kinect to collect RGB and depth images of the child's head.  Gaze directions can be tracked more accurately when head is viewed near frontal, and since we cared more about the child's attention to prompting agents than to sink objects, we setup the Kinect near the robot, and have sink objects further from the Kinect (i.e. soap and towel) be spread apart from each other to be easily distinguished.

The problem of estimating the object under child's attentional focus is broken into three parts: head pose estimation, eye pose estimation, and object identification.


%Overall objective
%	- to implement a real time gaze tracker
%a short reason why, so we know what's good enough and what's important to focus on
%
%2D webcam Approach:
%	- BLAH face tracker, eye corner extraction, stretch based inverse pose transformation, evaluation
%	
%	
%3D Kinect camera approach:
%	- KinFu mesh building, ICP head pose tracking, point cloud based inverse transformation, evaluation
%	- challenges of children with ASD footage, future algorithm requirement



\input{./tex/gaze_tracking_chp/head_pose_estimation}
\input{./tex/gaze_tracking_chp/eye_pose_estimation}
\input{./tex/gaze_tracking_chp/object_identification}

\section{Discussion}
We have shown working modification of the KinFu algorithm that produces frontal head pose transformed color eye images.  However, due to limitation of time, the head tracking algorithm was not evaluated.  We did some preliminary testing with the Kinect video footages we have obtained of the child with ASD during his hand-washing trials.  However, because the Kinect camera was placed too close to the participant's face (due to limitation of space near the sink), and because the participant rocks back and forth quite rapidly, our algorithm could not track the participant's head motions well.  Thus, we were not able to build a head model from the footages, and thus further evaluation (building frontal head pose transformed color eye images) was impeded.

For future studies, we need to first ensure the head tracker we developed works with children with ASD.  To do this, we should characterize children with ASD's head movement during hand-washing with the robot, since some children with ASD exhibit rocking motions.  We should note the speed and range of motions.  We may need to reposition the Kinect camera further away from the child to make sure the child's head motions does not go out of camera's range.  Then we need to characterize our head tracker's capability and make sure it can handle children with ASD's head movements.  If the head movements of children with ASD are faster than the tracker can handle, we may consider decreasing the number of vertices in the head model mesh to increase frame rate.

After obtaining a head tracker that works with children with ASD, we can move on to implementing the eye tracker.  Eye tracking training images can be obtained by using the head tracker, doing the frontal pose transformation, and cropping the eye region using the EYEDIAP dataset.  Then the eye tracker can be implemented following the ALR method outlined previously.

For the future studies, we should evaluate the gaze (head plus eye) tracking accuracy for children with ASD.  One way to evaluate it is by letting children with ASD wear a commercially available gaze tracking headset that gives the ground truth of their gaze, and compare it with the gaze direction estimated by our head plus eye tracker.  However, if asking children with ASD to wear the obtrusive headset proves infeasible, then maybe a fun activity can be designed instead that asks children with ASD to look at a moving object (e.g. a ball) whose 3D position can be automatically estimated using Kinect, serving as ground truth.

Lastly, future studies should integrate the head and eye tracker with object under gaze estimator to fully realize the VFOA tracker -- tracking the object under gaze in real-time.  Robot behaviors responding to child's gaze should then be implemented.  A study evaluating the effectiveness of this child's gaze behavior dependent robot prompting on children with ASD's engagement, prompt compliance, and step completion during hand-washing should be conducted.